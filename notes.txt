Bert from_pretrained :
    - bert-base-uncased : ~72%
        - Avec torch_dtype=torch.float16, attn_implementation="sdpa" : ~75%
        - truncation=True, padding=True : ~72%
        - model.eval() : ~72% (probablement le modèle par défaut)
    - bert-base-cased : 100%
    - bert-large-uncased : ~98%
    - bert-large-uncased-whole-word-masking : ~87%
    - bert-large-uncased-whole-word-masking-finetuned-squad : ~97%
    - bert-base-multilingual-uncased : ~68% (très long)(testé uniquement en anglais)
    - distilbert-base-uncased : ~76%
    - roberta-base : 100%
    - deberta-base : 100%

Encoder Decoder model : 
    - Bert2Bert : je n'ai pas réussi quoi que ce soit avec de modèle
    - Bart : top

Paramètre BertConfig() : 
    - hiden_size (multiple de 12): 
        plus grand -> nul
        plus petit -> bien mais pourquoi ??? valeur optimal au alentour de 12*32
        - Non testable sur bert-large : BertConfig est paramétré pour bert-base, pas de solution pour l'instant pour bert-large
    - position_embedding_type : non compatible avec BertSdpaSelfAttention utilisé dans torch

Test de distance : 
    n = 2000
    hidden_size = 12*32
    - random word : 
        Moyenne: 1.0014798277114427
        Médiane: 1.0012299817117036
        Premier quartile (Q1): 0.9674614874170349
        Troisième quartile (Q3): 1.0371143882599176
    - 'Hello :
        Moyenne: 1.0007133102729706
        Médiane: 1.001409812445253
        Premier quartile (Q1): 0.9673643592760108
        Troisième quartile (Q3): 1.0349812603785673

    hidden_size = 12*64
    - random word :
        Moyenne: 1.0003106674610307
        Médiane: 1.0003716082031953
        Premier quartile (Q1): 0.9772424083377091
        Troisième quartile (Q3): 1.0249368293436683
    - 'Hello' :
        Moyenne: 1.0003069387206316
        Médiane: 1.0003347508609295
        Premier quartile (Q1): 0.9757540288061908
        Troisième quartile (Q3): 1.0251342274156077


Bart : 
    - bart-large : 
        - vocab_size : 50265
        - embedding_size : 1024
    - bart-base :
        - vocab_size : 50265
        - embedding_size : 768

    - Paramètre :
        - num_beam = 1
        - early_stopping = False

    - Tokenizer : BartTokenizer = AutoTokenizer

    - Valuer min vecteur embedding -2.1
    - Valeur max vecteur embedding 1.4

    - Norme :
        - min : 4.3
        - max : 5.07

    - Test voisins (4 pts) - bart-base :
        - 'coffee' (2 tokens):
            - Distance 1 : ['coffee']
            - Distance 2 : ['', 'coffeecoffeeffeecocoffee', 'advertisement', 'coffeecoffee', 'coffee']
            - Distance 3 : ['', 'coffeecoffeeffeecocoffee', 'advertisement', 'coffeecoffeeffeecocoffeecffeecoffffeeco coffeecoffee', 'coffeecoffeeffeecocoffeescoffee', 'coffeecoffee', 'coffee', 'coffeecoffeeffeecocoffeewatercoffeecffeecoffecoffeetoffeecooffcoffeeoffeeco-coffee']
        - 'hello' (1 token): 
            - Distance 1 : ['hellohello', 'hello']
            - Distance 2 : ['', 'hello', 'advertisement', 'hellohello', 'hellohellohello']
            - Distance 3 : ['', 'hello', 'advertisement', 'hellohellohello', 'hellohello']

    - Test voisins (4 pts) - bart-large :
        - 'coffee' (2 tokens):
            - Distance 1 : ['coffee']
            - Distance 2 : ['coffeecoffee', 'coffee.coffee', 'coffee']
            - Distance 3 : ['', 'cocoffee', 'cobee', 'concaffe.coffee', 'coffee.coffee', 'coffee', ',', 'coffee.', 'coffee.coffee-mocha.co.co', 'coffeecoffee', 'coffeeco.', 'coff', 'coag', 'coffee www.coffee', 'cocoffeecoffee.coffeewww.coffe.cocoffewww.coffeecoffee', 'coffeewww.coffee', '.www.coffee.com/blog.com.co.za', 'co-', 'coffee.coffeecoffeewww.coffe.co.cocoffee', 'cococof ( ( ( T ( ( A ( ( and ( ( M ( ( B ( ( really ( ( D ( ( " ( ( R ( ( I ( ( O ( ( definitely ( ( St ( ( Y', 'co.co']
   
       - Test cecle :
        - 4 pts : 2n -> 1024 = 2048 vecteur
        - 8 pts : 3^n -> 1024 = 3,733918487e+488