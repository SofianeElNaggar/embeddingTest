Bert from_pretrained :
    - bert-base-uncased : ~72%
        - Avec torch_dtype=torch.float16, attn_implementation="sdpa" : ~75%
        - truncation=True, padding=True : ~72%
        - model.eval() : ~72% (probablement le modèle par défaut)
    - bert-base-cased : 100%
    - bert-large-uncased : ~98%
    - bert-large-uncased-whole-word-masking : ~87%
    - bert-large-uncased-whole-word-masking-finetuned-squad : ~97%
    - bert-base-multilingual-uncased : ~68% (très long)(testé uniquement en anglais)
    - distilbert-base-uncased : ~76%
    - roberta-base : 100%
    - deberta-base : 100%

Encoder Decoder model : 
    - Bert2Bert : je n'ai pas réussi quoi que ce soit avec de modèle
    - Bart : top

Paramètre BertConfig() : 
    - hiden_size (multiple de 12): 
        plus grand -> nul
        plus petit -> bien mais pourquoi ??? valeur optimal au alentour de 12*32
        - Non testable sur bert-large : BertConfig est paramétré pour bert-base, pas de solution pour l'instant pour bert-large
    - position_embedding_type : non compatible avec BertSdpaSelfAttention utilisé dans torch

Test de distance : 
    n = 2000
    hidden_size = 12*32
    - random word : 
        Moyenne: 1.0014798277114427
        Médiane: 1.0012299817117036
        Premier quartile (Q1): 0.9674614874170349
        Troisième quartile (Q3): 1.0371143882599176
    - 'Hello :
        Moyenne: 1.0007133102729706
        Médiane: 1.001409812445253
        Premier quartile (Q1): 0.9673643592760108
        Troisième quartile (Q3): 1.0349812603785673

    hidden_size = 12*64
    - random word :
        Moyenne: 1.0003106674610307
        Médiane: 1.0003716082031953
        Premier quartile (Q1): 0.9772424083377091
        Troisième quartile (Q3): 1.0249368293436683
    - 'Hello' :
        Moyenne: 1.0003069387206316
        Médiane: 1.0003347508609295
        Premier quartile (Q1): 0.9757540288061908
        Troisième quartile (Q3): 1.0251342274156077


Bart : 
    - bart-large : 
        - vocab_size : 50265
        - embedding_size : 1024
    - bart-base :
        - vocab_size : 50265
        - embedding_size : 768

    - Paramètre :
        - num_beam = 1
        - early_stopping = False

    - Tokenizer : BartTokenizer == AutoTokenizer

    - Valuer min vecteur embedding -2.1
    - Valeur max vecteur embedding 1.4

    - Norme :
        - min : 4.3
        - max : 5.07

    - Test voisins (4 pts) - bart-base :
        - 'coffee' (2 tokens):
            - Distance 1 : {'coffee': 1536}
            - Distance 2 : {'coffee': 1530, 'advertisement': 1, 'coffeecoffee': 3, 'coffeecoffeeffeecocoffee': 1, '': 1}
            - Distance 3 : {'coffee': 1523, 'advertisement': 2, 'coffeecoffeeffeecocoffeecffeecoffffeeco coffeecoffee': 1, 'coffeecoffeeffeecocoffee': 3, 'coffeecoffeeffeecocoffeescoffee': 1, '': 2, 'coffeecoffee': 3, 'coffeecoffeeffeecocoffeewatercoffeecffeecoffecoffeetoffeecooffcoffeeoffeeco-coffee': 1}
        - 'hello' (1 token): 
            - Distance 1 : {'hello': 1534, 'hellohello': 2}
            - Distance 2 : {'hello': 1525, 'advertisement': 1, 'hellohellohello': 5, 'hellohello': 3, '': 2}
            - Distance 3 : {'hello': 1517, 'advertisement': 3, 'hellohellohello': 10, 'hellohello': 4, '': 2}

    - Test voisins (4 pts) - bart-large :
        - 'coffee' (2 tokens) (bien bien long):
            - Distance 1 : {'coffee': 2048}
            - Distance 2 : {'coffee': 2044, 'coffee.coffee': 3, 'coffeecoffee': 1}
            - Distance 3 : {'coffee': 1970, 'coffee.coffee': 26, '': 21, 'coffee.': 12, 'coffeecoffee': 3, 'coff': 1, 'cococof ( ( ( T ( ( A ( ( and ( ( M ( ( B ( ( really ( ( D ( ( " ( ( R ( ( I ( ( O ( ( definitely ( ( St ( ( Y': 1, 'cocoffeecoffee.coffeewww.coffe.cocoffewww.coffeecoffee': 1, 'coffeewww.coffee': 1, 'coffee www.coffee': 1, 'coffeeco.': 1, 'coffee.coffee-mocha.co.co': 1, 'coag': 1, 'coffee.coffeecoffeewww.coffe.co.cocoffee': 1, 'cocoffee': 1, 'co-': 1, ',': 1, 'concaffe.coffee': 1, 'cobee': 1, 'co.co': 1, '.www.coffee.com/blog.com.co.za': 1}
        - 'hello' (1 token):
            - Distance 1 : {'hello': 2047, 'hellohello': 1}
            - Distance 2 : {'hello': 2044, '': 2, 'hellohello': 1, 'hellohellohellohellohello': 1}
            - Distance 3 : {'hello': 2007, 'hello.hello': 2, '': 24, 'SOURCE:': 2, 'hellohello': 7, 'hellohellohellohellohello': 1, 'hellohellohello': 1, 'SOURCE: www.thedailybeast.com': 1, 'SOURCE: www.hello': 1, 'mayhem@postmedia.com': 1, 'SOURCE: http://www.worldnews.com.au/news/world-news/2017/07/27/world/world_news_news/': 1}

    - Test voisins cercle :
        - 4 pts : 2n -> 1024 = 2048 vecteurs
        - 8 pts : 3^n -> 1024 = 3,733918487e+488 vecteurs
    
    - Test interpolation : 
        - random_interpolate_test() : ~2.5 results/min
        - Binôme "interessant" :
            - west - nature
            - stroke - rest
            - add - button
            - online - search