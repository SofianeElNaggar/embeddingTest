Bert from_pretrained :
    - bert-base-uncased : ~72%
        - Avec torch_dtype=torch.float16, attn_implementation="sdpa" : ~75%
        - truncation=True, padding=True : ~72%
        - model.eval() : ~72% (probablement le modÃ¨le par dÃ©faut)
    - bert-base-cased : 100%
    - bert-large-uncased : ~98%
    - bert-large-uncased-whole-word-masking : ~87%
    - bert-large-uncased-whole-word-masking-finetuned-squad : ~97%
    - bert-base-multilingual-uncased : ~68% (trÃ¨s long)(testÃ© uniquement en anglais)
    - distilbert-base-uncased : ~76%
    - roberta-base : 100%
    - deberta-base : 100%

Encoder Decoder model : 
    - Bert2Bert : je n'ai pas rÃ©ussi quoi que ce soit avec de modÃ¨le
    - Bart : top

ParamÃ¨tre BertConfig() : 
    - hiden_size (multiple de 12): 
        plus grand -> nul
        plus petit -> bien mais pourquoi ??? valeur optimal au alentour de 12*32
        - Non testable sur bert-large : BertConfig est paramÃ©trÃ© pour bert-base, pas de solution pour l'instant pour bert-large
    - position_embedding_type : non compatible avec BertSdpaSelfAttention utilisÃ© dans torch

Test de distance : 
    n = 2000
    hidden_size = 12*32
    - random word : 
        Moyenne: 1.0014798277114427
        MÃ©diane: 1.0012299817117036
        Premier quartile (Q1): 0.9674614874170349
        TroisiÃ¨me quartile (Q3): 1.0371143882599176
    - 'Hello :
        Moyenne: 1.0007133102729706
        MÃ©diane: 1.001409812445253
        Premier quartile (Q1): 0.9673643592760108
        TroisiÃ¨me quartile (Q3): 1.0349812603785673

    hidden_size = 12*64
    - random word :
        Moyenne: 1.0003106674610307
        MÃ©diane: 1.0003716082031953
        Premier quartile (Q1): 0.9772424083377091
        TroisiÃ¨me quartile (Q3): 1.0249368293436683
    - 'Hello' :
        Moyenne: 1.0003069387206316
        MÃ©diane: 1.0003347508609295
        Premier quartile (Q1): 0.9757540288061908
        TroisiÃ¨me quartile (Q3): 1.0251342274156077

Bart :
    - bart-large : 
        - vocab_size : 50265
        - embedding_size : 1024
    - bart-base :
        - vocab_size : 50265
        - embedding_size : 768

    - Vocab : 
        - mot commenÃ§ant par Ä  = mot prÃ©cÃ©dÃ© d'un espace
        - Pour bart-large, les mots prÃ©cÃ©dÃ© d'un espace seront soit transcrit en "" soit, un autre mot sera ajoutÃ© devant 
          ex : ' coffee' -> "" | " Coffee" -> "Mr. Coffee"
        - Pour bart-base, la majoritÃ© des mots prÃ©cÃ©dÃ© d'un espace seront transcrit en advertisement, pourquoi ? ðŸ¤·

    - ParamÃ¨tre :
        - num_beam = 1
        - early_stopping = False

    - Tokenizer : BartTokenizer == AutoTokenizer

    - Valuer min vecteur embedding -2.1
    - Valeur max vecteur embedding 1.4

    - Norme :
        - min : 4.3
        - max : 5.07

    - Test voisins (4 pts) - bart-base :
        - 'coffee' (2 tokens):
            - Distance 1 : {'coffee': 1536}
            - Distance 2 : {'coffee': 1530, 'advertisement': 1, 'coffeecoffee': 3, 'coffeecoffeeffeecocoffee': 1, '': 1}
            - Distance 3 : {'coffee': 1523, 'advertisement': 2, 'coffeecoffeeffeecocoffeecffeecoffffeeco coffeecoffee': 1, 'coffeecoffeeffeecocoffee': 3, 'coffeecoffeeffeecocoffeescoffee': 1, '': 2, 'coffeecoffee': 3, 'coffeecoffeeffeecocoffeewatercoffeecffeecoffecoffeetoffeecooffcoffeeoffeeco-coffee': 1}
        - ' coffee' (1 token): ' coffee' est traduit en 'advertisement' par decode_embedding() -> premiÃ¨re erreur remarquÃ© sur le dÃ©code
            - Distance 0.5 : {'advertisement': 1536}
            - Distance 1 : {'advertisement': 1535, ' coffee': 1}
            - Distance 1.5 : {'advertisement': 1532, ' coffee coffee coffee': 4}
        - 'hello' (1 token): 
            - Distance 1 : {'hello': 1534, 'hellohello': 2}
            - Distance 2 : {'hello': 1525, 'advertisement': 1, 'hellohellohello': 5, 'hellohello': 3, '': 2}
            - Distance 3 : {'hello': 1517, 'advertisement': 3, 'hellohellohello': 10, 'hellohello': 4, '': 2}
        - 'Advertisement' :
            - Distance 1 : {'Advertisement': 1534, 'advertisement': 2}
            - Distance 2 : {'Advertisement': 1524, 'advertisement': 8, 'AdvertisementAdvertisementAdvertisement': 2, 'AdvertisementAdvertisement': 2}
            - Distance 3 : {'Advertisement': 1499, 'advertisement': 29, 'AdvertisementAdvertisementAdvertisement': 7, 'AdvertisementAdvertisement': 1}
        - 'advertisement' :
            - Distance 1 : {'advertisement': 1536}
            - Distance 2 : {'advertisement': 1531, 'advertisementadvertisementadvertisement': 3, 'advertisementadvertisement': 2}
            - Distance 3 : {'advertisement': 1530, 'advertisementadvertisementadvertisement': 6}

    - Test voisins (4 pts) - bart-large :
        - 'coffee' (2 tokens) (bien bien long):
            - Distance 1 : {'coffee': 2048}
            - Distance 2 : {'coffee': 2044, 'coffee.coffee': 3, 'coffeecoffee': 1}
            - Distance 3 : {'coffee': 1970, 'coffee.coffee': 26, '': 21, 'coffee.': 12, 'coffeecoffee': 3, 'coff': 1, 'cococof ( ( ( T ( ( A ( ( and ( ( M ( ( B ( ( really ( ( D ( ( " ( ( R ( ( I ( ( O ( ( definitely ( ( St ( ( Y': 1, 'cocoffeecoffee.coffeewww.coffe.cocoffewww.coffeecoffee': 1, 'coffeewww.coffee': 1, 'coffee www.coffee': 1, 'coffeeco.': 1, 'coffee.coffee-mocha.co.co': 1, 'coag': 1, 'coffee.coffeecoffeewww.coffe.co.cocoffee': 1, 'cocoffee': 1, 'co-': 1, ',': 1, 'concaffe.coffee': 1, 'cobee': 1, 'co.co': 1, '.www.coffee.com/blog.com.co.za': 1}
        - ' coffee' (1 token):
            - Distance 1 :
        - 'hello' (1 token):
            - Distance 1 : {'hello': 2047, 'hellohello': 1}
            - Distance 2 : {'hello': 2044, '': 2, 'hellohello': 1, 'hellohellohellohellohello': 1}
            - Distance 3 : {'hello': 2007, 'hello.hello': 2, '': 24, 'SOURCE:': 2, 'hellohello': 7, 'hellohellohellohellohello': 1, 'hellohellohello': 1, 'SOURCE: www.thedailybeast.com': 1, 'SOURCE: www.hello': 1, 'mayhem@postmedia.com': 1, 'SOURCE: http://www.worldnews.com.au/news/world-news/2017/07/27/world/world_news_news/': 1}

    - Test voisins cercle :
        - 4 pts : 2n -> 1024 = 2048 vecteurs
        - 8 pts : 3^n -> 1024 = 3,733918487e+488 vecteurs
    
    - Test interpolation : 
        - random_interpolate_test() : 
            - cpu : ~2.5 results/min
            - cuda : ~15 results/min

- Embedding de code : 
    - codeBert : nop
    - codeParrot : bof
    - codeGen : bof 
    - 

OpenAI APIkey : sk-None-55QIFzsu0vVcaPaMAHDoT3BlbkFJKxA9X07vkg0cGr0rhB3k