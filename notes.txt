Bert from_pretrained :
    - bert-base-uncased : ~72%
        - Avec torch_dtype=torch.float16, attn_implementation="sdpa" : ~75%
        - truncation=True, padding=True : ~72%
        - model.eval() : ~72% (probablement le modèle par défaut)
    - bert-base-cased : 100%
    - bert-large-uncased : ~98%
    - bert-large-uncased-whole-word-masking : ~87%
    - bert-large-uncased-whole-word-masking-finetuned-squad : ~97%
    - bert-base-multilingual-uncased : ~68% (très long)(testé uniquement en anglais)
    - distilbert-base-uncased : ~76%
    - roberta-base : 100%
    - deberta-base : 100%

Encoder Decoder model : 
    - Bert2Bert : je n'ai pas réussi quoi que ce soit avec de modèle
    - Bart : top

Paramètre BertConfig() : 
    - hiden_size (multiple de 12): 
        plus grand -> nul
        plus petit -> bien mais pourquoi ??? valeur optimal au alentour de 12*32
        - Non testable sur bert-large : BertConfig est paramétré pour bert-base, pas de solution pour l'instant pour bert-large
    - position_embedding_type : non compatible avec BertSdpaSelfAttention utilisé dans torch

Test de distance : 
    n = 2000
    hidden_size = 12*32
    - random word : 
        Moyenne: 1.0014798277114427
        Médiane: 1.0012299817117036
        Premier quartile (Q1): 0.9674614874170349
        Troisième quartile (Q3): 1.0371143882599176
    - 'Hello :
        Moyenne: 1.0007133102729706
        Médiane: 1.001409812445253
        Premier quartile (Q1): 0.9673643592760108
        Troisième quartile (Q3): 1.0349812603785673

    hidden_size = 12*64
    - random word :
        Moyenne: 1.0003106674610307
        Médiane: 1.0003716082031953
        Premier quartile (Q1): 0.9772424083377091
        Troisième quartile (Q3): 1.0249368293436683
    - 'Hello' :
        Moyenne: 1.0003069387206316
        Médiane: 1.0003347508609295
        Premier quartile (Q1): 0.9757540288061908
        Troisième quartile (Q3): 1.0251342274156077


Bart : 
    - bart-large : 
        - vocab_size : 50265
        - embedding_size : 1024
    - bart-base :
        - vocab_size : 50265
        - embedding_size : 768

    - Paramètre :
        - num_beam = 1
        - early_stopping = False

    - Valuer min vecteur embedding -2.1
    - Valeur max vecteur embedding 1.4

    - Norme :
        - min : 4.3
        - max : 5.07

    - Test variants (4 pts) - bart-large - AutoTokenizer:
        - 'Hello :
            - Distance 1 : ['hello', 'hellohello']
            - Distance 2 : ['', 'hellohellohellohellohello', 'hellohello', 'hello']
            - Distance 4 : ['', 'SOURCE:', '.', 'hello.hello.', 'hellohellohello', 'hellohellohellowwwhellohello.hellohelloSOURCEhellohellomayhellohellohttphellohello', 'hello', "'", 'hellohello', 'm.m.gisak@tribpublishing.com', 'SOURCE: www.matthewmccall.com.', 't', 'ak.gofundefined', 'deal', 't.m.', 'SOURCE: http://www.huffingtonpost.com/', 'www.happen.com.uk/news/world/world-news/', 'hellohellohellohellohello', 'www.hometown', 'Copyright 2016', 'hello.', 'SOURCE: www.theguardian.com.hello', 'SOURCE: www.truro.com', 'The www.cbsnews.com/news/local/2018/11/19/cbs-news/', 'SOURCE', 'hello.hello', 'gfang@tribune.com, https://www.wj.com/']
            -
        - 'coffee' :
            - Distance 1 : ['coffee']
            - Distance 2 : ['coffee', 'coffee.coffee']
            - Distance 3 : ['', 'coffee', 'coffee.coffee', 'www.coffee.com/news/news-and-politics/news_story/news.story?id=7', 'coffee.coffeecoffee', 'coco', 'coffeecoffee']
    
    - Test variants (4 pts) - bart-base - AutoTokenizer:
        - 'coffee' :
            - Distance 1 : ['coffee']
            - Distance 2 : ['cococoffee', 'coffeecoffee', 'coffee']
            - Distance 3 : ['', 'coffeecoffeeffeecocoffeewww.coffee.co.coco.comcoffee', 'cococoffeecoco-coco', 'cococoffeecoco-coco:cocofococoococochococo', 'coffeecoffeeffeecocoffee', 'www.coffee', 'advertisement', 'cococowwwcocoscocomorecocohttpcocotococo', 'cococoffeecocowwwcoco:coco://coco :cocoococohttpcocofcocolitecocofriedcocowatercocopetcococoxcococovercocoochco', 'coffeecoffeedelcococoffeeffeecoco coffeecoffee coffeecocolecoffee', 'coffeecoffee', 'cococoffeecococcoco cococofcoco-cocowww.coco.coffee.co.comcoco:cocochococo://coco', 'cococoffeecoco.cocoffecoffee', 'coffee']

    - Test variants (4 pts) - bart-base - BartTokenizer:
        - 'coffee' :
            - Distance 1 : ['coffee']
            - Distance 2 : ['cococoffeecoco', 'coffeecoffee', 'coffee']
            - Distance 3 : ['', 'coffee', 'coffeecoffee', 'cococoffeecoco-coco:coco.cocowww.coffee.co.comcoco cococo', 'cococoffeecoco-coco://coco.coco:cocowww.coffee.co.comcocohttpcoco :cocoococolitecocoffecocochocococox', 'cococowwwcocoscoco', 'cococoffee']

    - Test cecle :
        - 4 pts : 2n -> 1024 = 2048 vecteur
        - 8 pts : 3^n -> 1024 = 3,733918487e+488